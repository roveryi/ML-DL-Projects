{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"Project1_Question7_Grid_Search.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KWezNU9WgLZw","colab_type":"text"},"source":["## Question 7: Pipeline and Grid Search"]},{"cell_type":"code","metadata":{"id":"uAyD3rXAgLZ2","colab_type":"code","outputId":"bae2c225-5f64-440a-af99-92175a703830","executionInfo":{"status":"ok","timestamp":1579697099227,"user_tz":480,"elapsed":6534,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["#import all packages\n","import numpy as np\n","import random\n","import re\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk import pos_tag\n","import matplotlib.pyplot as plt\n","from nltk.stem.wordnet import WordNetLemmatizer\n","#all sklearn\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction import text\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn import svm\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.decomposition import NMF\n","from sklearn.pipeline import Pipeline\n","# used to cache results\n","import time\n","from tempfile import mkdtemp\n","from shutil import rmtree\n","from sklearn.externals.joblib import Memory\n","import pickle\n","\n","#prepare two dataset (remove header v.s. not remove)\n","computer_technology = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n","recreational_activity = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n","\n","#remove_header\n","comp_train_remove = fetch_20newsgroups(subset='train', categories=computer_technology, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n","rec_train_remove = fetch_20newsgroups(subset='train', categories=recreational_activity, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n","comp_test_remove = fetch_20newsgroups(subset='test', categories=computer_technology, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n","rec_test_remove = fetch_20newsgroups(subset='test', categories=recreational_activity, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n","\n","#not remove\n","comp_train = fetch_20newsgroups(subset='train', categories=computer_technology, shuffle=True, random_state=42)\n","rec_train = fetch_20newsgroups(subset='train', categories=recreational_activity, shuffle=True, random_state=42)\n","comp_test = fetch_20newsgroups(subset='test', categories=computer_technology, shuffle=True, random_state=42)\n","rec_test = fetch_20newsgroups(subset='test', categories=recreational_activity, shuffle=True, random_state=42)\n","\n","# map to binary classification\n","#remove_header\n","all_train_remove = comp_train_remove.data+rec_train_remove.data\n","all_test_remove = comp_test_remove.data+rec_test_remove.data\n","target_train_remove = [True] * len(comp_train_remove.data) + [False] * len(rec_train_remove.data)\n","target_test_remove = [True] * len(comp_test_remove.data) + [False] * len(rec_test_remove.data)\n","\n","#not remove\n","all_train = comp_train.data+rec_train.data\n","all_test = comp_test.data+rec_test.data\n","target_train = [True] * len(comp_train.data) + [False] * len(rec_train.data)\n","target_test = [True] * len(comp_test.data) + [False] * len(rec_test.data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rIdF2-dzgLZ_","colab_type":"code","colab":{}},"source":["#Define functions for pipeline\n","#Feature Extraction\n","min_df = [3, 5]\n","#lemmatization_tokenizer\n","def penn2morphy(penntag):\n","    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n","    morphy_tag = {'NN':'n', 'JJ':'a',\n","                  'VB':'v', 'RB':'r'}\n","    try:\n","        return morphy_tag[penntag[:2]]\n","    except:\n","        return 'n'\n","    \n","def lemmatized_tokenizer(text):\n","    wnl = WordNetLemmatizer()\n","    clean_text = re.sub(r'[^A-Za-z]', \" \", text)\n","    tokenized_text = nltk.word_tokenize(clean_text) \n","    lemaitzed_text = [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) for word, tag in pos_tag(tokenized_text)]\n","    return lemaitzed_text\n","\n","#general_tokenizer\n","def tokenizer(text):\n","    clean_text = re.sub(r'[^A-Za-z]', \" \", text)\n","    tokenized_text = nltk.word_tokenize(clean_text)\n","    return tokenized_text\n","#---------------------------------------------------------------------#\n","#Dimensionality Reduction\n","#LSI\n","svd = TruncatedSVD(n_components=50, random_state=0)\n","#NMF\n","nmf = NMF(n_components=50, init='random', random_state=0)\n","#---------------------------------------------------------------------#\n","#Classifier\n","#Support Vector Machine\n","svm_clf = svm.SVC(probability=True,gamma=1) #best svm gamma = 1\n","#Logistic Regression (L1 and L2)\n","logistic_l1_clf = LogisticRegression(penalty='l1',C=10) #est l1 c=10\n","logistic_l2_clf = LogisticRegression(penalty='l2',C=100) #best l2 c=100\n","#Gaussian Naive Bayes\n","GaussianNB_clf = GaussianNB()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"wFbAZluBgLaE","colab_type":"code","outputId":"f1567cb5-fa69-413e-ed4e-7ce54b4d6d82","executionInfo":{"status":"error","timestamp":1579648023397,"user_tz":480,"elapsed":6759283,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Pipeline for remove header...etc\n","#traing and evaluate the pipeline\n","print(\"Cell started\")\n","start = time.time()\n","\n","cachedir = mkdtemp()\n","memory = Memory(cachedir=cachedir, verbose=10)\n","print(\"Building Pipeline\")\n","#remove_lemmatized\n","pipeline1 = Pipeline([\n","    ('vect', CountVectorizer(stop_words='english', tokenizer=lemmatized_tokenizer)),\n","    ('tfidf', TfidfTransformer()),\n","    ('reduce_dim', TruncatedSVD(n_components=50,random_state=0)),\n","    ('clf', GaussianNB()),\n","])\n","#remove_tokenizer\n","pipeline2 = Pipeline([\n","    ('vect', CountVectorizer(stop_words='english', tokenizer=tokenizer)),\n","    ('tfidf', TfidfTransformer()),\n","    ('reduce_dim', TruncatedSVD(n_components=50,random_state=0)),\n","    ('clf', GaussianNB()),\n","])\n","#not-remove_lemmatized\n","pipeline3 = Pipeline([\n","    ('vect', CountVectorizer(stop_words='english', tokenizer=lemmatized_tokenizer)),\n","    ('tfidf', TfidfTransformer()),\n","    ('reduce_dim', TruncatedSVD(n_components=50,random_state=0)),\n","    ('clf', GaussianNB()),\n","])\n","#not-remove_tokenizer\n","pipeline4 = Pipeline([\n","    ('vect', CountVectorizer(stop_words='english', tokenizer=tokenizer)),\n","    ('tfidf', TfidfTransformer()),\n","    ('reduce_dim', TruncatedSVD(n_components=50,random_state=0)),\n","    ('clf', GaussianNB()),\n","])\n","param_grid = [\n","     {\n","         'vect__min_df': min_df,\n","         'reduce_dim': [svd, nmf],\n","         'clf': [svm_clf, logistic_l1_clf, logistic_l2_clf, GaussianNB_clf]\n","     }\n","]\n","\n","grid1 = GridSearchCV(pipeline1, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n","grid2 = GridSearchCV(pipeline2, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n","#grid3 = GridSearchCV(pipeline3, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n","#grid4 = GridSearchCV(pipeline4, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n","\n","# Fit REMOVED HEADERS AND FOOTERS, LEMMITIZED\n","print(\"Fitting grid 1...\")\n","t1 = time.time()\n","grid1.fit(all_train_remove, target_train_remove)\n","print(\"Fit grid 1 in %f sec\" % (time.time()-t1))\n","\n","print(\"Fitting grid 2...\")\n","t1 = time.time()\n","# Fit REMOVED HEADERS AND FOOTERS, NOT LEMMITIZED\n","grid2.fit(all_train_remove, target_train_remove)\n","print(\"Fit grid 2 in %f sec\" % (time.time()-t1))\n","\n","print(\"Fitting grid 3...\")\n","t1 = time.time()\n","# Fit KEPT HEADERS AND FOOTERS, LEMMITIZED\n","grid3.fit(all_train, target_train)\n","print(\"Fit grid 3 in %f sec\" % (time.time()-t1))\n","\n","print(\"Fitting grid 4...\")\n","t1 = time.time()\n","# Fit KEPT HEADERS AND FOOTERS, NOT LEMMITIZED\n","grid4.fit(all_train, target_train)\n","print(\"Fit grid 4 in %f sec\" % (time.time()-t1))\n","\n","pickle.dump( grid1, open(\"grid1.pkl\",\"wb\"))\n","pickle.dump( grid2, open(\"grid2.pkl\",\"wb\"))\n","pickle.dump( grid3, open(\"grid3.pkl\",\"wb\"))\n","pickle.dump( grid4, open(\"grid4.pkl\",\"wb\"))\n","\n","rmtree(cachedir)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cell started\n","Building Pipeline\n","Fitting grid 1...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n","You provided \"cachedir='/tmp/tmpo_oo2x6g'\", use \"location='/tmp/tmpo_oo2x6g'\" instead.\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Fit grid 1 in 4079.723052 sec\n","Fitting grid 2...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Fit grid 2 in 825.662278 sec\n","Fitting grid 3...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-4bd076d35ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Fit KEPT HEADERS AND FOOTERS, LEMMITIZED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mgrid3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fit grid 3 in %f sec\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-bd09a73e4bf3>\u001b[0m in \u001b[0;36mlemmatized_tokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^A-Za-z]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlemaitzed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenn2morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemaitzed_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mprev2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ObftdqgUgLaL","colab_type":"code","outputId":"8c6cd082-dfc1-45f7-fc23-9d9a0315aae1","executionInfo":{"status":"ok","timestamp":1579648186450,"user_tz":480,"elapsed":959,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import pandas as pd\n","#remove header, lemmatized\n","pd.DataFrame(grid1.cv_results_)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_clf</th>\n","      <th>param_reduce_dim</th>\n","      <th>param_vect__min_df</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>split4_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>40.640022</td>\n","      <td>0.898973</td>\n","      <td>9.453645</td>\n","      <td>0.992695</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.923970</td>\n","      <td>0.951426</td>\n","      <td>0.930233</td>\n","      <td>0.942918</td>\n","      <td>0.917548</td>\n","      <td>0.933219</td>\n","      <td>0.012374</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>40.390951</td>\n","      <td>1.062882</td>\n","      <td>9.457797</td>\n","      <td>1.025638</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.927138</td>\n","      <td>0.948258</td>\n","      <td>0.929175</td>\n","      <td>0.945032</td>\n","      <td>0.920719</td>\n","      <td>0.934064</td>\n","      <td>0.010693</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>52.366115</td>\n","      <td>2.147812</td>\n","      <td>9.626980</td>\n","      <td>1.061701</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.885956</td>\n","      <td>0.898627</td>\n","      <td>0.887949</td>\n","      <td>0.898520</td>\n","      <td>0.861522</td>\n","      <td>0.886515</td>\n","      <td>0.013549</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>50.632540</td>\n","      <td>0.802019</td>\n","      <td>9.557514</td>\n","      <td>1.048083</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.899683</td>\n","      <td>0.911299</td>\n","      <td>0.896406</td>\n","      <td>0.909091</td>\n","      <td>0.891121</td>\n","      <td>0.901520</td>\n","      <td>0.007624</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>37.942688</td>\n","      <td>1.089744</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>37.900110</td>\n","      <td>1.126926</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>47.175756</td>\n","      <td>2.060629</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>46.071314</td>\n","      <td>0.696679</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>38.055257</td>\n","      <td>1.133176</td>\n","      <td>9.444636</td>\n","      <td>1.073403</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.932418</td>\n","      <td>0.956705</td>\n","      <td>0.933404</td>\n","      <td>0.952431</td>\n","      <td>0.935518</td>\n","      <td>0.942095</td>\n","      <td>0.010322</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>38.142144</td>\n","      <td>1.007880</td>\n","      <td>9.457181</td>\n","      <td>1.011294</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.930306</td>\n","      <td>0.957761</td>\n","      <td>0.934461</td>\n","      <td>0.950317</td>\n","      <td>0.936575</td>\n","      <td>0.941884</td>\n","      <td>0.010398</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>47.492243</td>\n","      <td>2.189090</td>\n","      <td>9.516887</td>\n","      <td>1.030335</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.918691</td>\n","      <td>0.938754</td>\n","      <td>0.926004</td>\n","      <td>0.935518</td>\n","      <td>0.910148</td>\n","      <td>0.925823</td>\n","      <td>0.010562</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>46.306815</td>\n","      <td>0.852042</td>\n","      <td>9.426680</td>\n","      <td>1.048520</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.921859</td>\n","      <td>0.947202</td>\n","      <td>0.919662</td>\n","      <td>0.941860</td>\n","      <td>0.921776</td>\n","      <td>0.930472</td>\n","      <td>0.011630</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>37.995104</td>\n","      <td>0.963981</td>\n","      <td>9.440788</td>\n","      <td>1.022973</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.776135</td>\n","      <td>0.828933</td>\n","      <td>0.787526</td>\n","      <td>0.791755</td>\n","      <td>0.790698</td>\n","      <td>0.795009</td>\n","      <td>0.017845</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>37.955596</td>\n","      <td>0.953421</td>\n","      <td>9.417252</td>\n","      <td>1.045805</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.775079</td>\n","      <td>0.831045</td>\n","      <td>0.771670</td>\n","      <td>0.806554</td>\n","      <td>0.798097</td>\n","      <td>0.796489</td>\n","      <td>0.021783</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>47.389523</td>\n","      <td>2.184358</td>\n","      <td>9.490765</td>\n","      <td>1.072492</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.898627</td>\n","      <td>0.917635</td>\n","      <td>0.904863</td>\n","      <td>0.911205</td>\n","      <td>0.891121</td>\n","      <td>0.904690</td>\n","      <td>0.009284</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>46.151379</td>\n","      <td>0.805346</td>\n","      <td>9.506888</td>\n","      <td>1.018021</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.902851</td>\n","      <td>0.930306</td>\n","      <td>0.905920</td>\n","      <td>0.917548</td>\n","      <td>0.892178</td>\n","      <td>0.909760</td>\n","      <td>0.013072</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n","0       40.640022      0.898973  ...        0.012374                4\n","1       40.390951      1.062882  ...        0.010693                3\n","2       52.366115      2.147812  ...        0.013549               10\n","3       50.632540      0.802019  ...        0.007624                9\n","4       37.942688      1.089744  ...             NaN               13\n","5       37.900110      1.126926  ...             NaN               14\n","6       47.175756      2.060629  ...             NaN               15\n","7       46.071314      0.696679  ...             NaN               16\n","8       38.055257      1.133176  ...        0.010322                1\n","9       38.142144      1.007880  ...        0.010398                2\n","10      47.492243      2.189090  ...        0.010562                6\n","11      46.306815      0.852042  ...        0.011630                5\n","12      37.995104      0.963981  ...        0.017845               12\n","13      37.955596      0.953421  ...        0.021783               11\n","14      47.389523      2.184358  ...        0.009284                8\n","15      46.151379      0.805346  ...        0.013072                7\n","\n","[16 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"UzjAmEjygLaR","colab_type":"code","outputId":"52cfa225-d090-47fb-89c5-1cbabef4053e","executionInfo":{"status":"ok","timestamp":1579648244122,"user_tz":480,"elapsed":969,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#remove header, not lemmatized\n","pd.DataFrame(grid2.cv_results_)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_clf</th>\n","      <th>param_reduce_dim</th>\n","      <th>param_vect__min_df</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>split4_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.616890</td>\n","      <td>0.029710</td>\n","      <td>0.930541</td>\n","      <td>0.049903</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.918691</td>\n","      <td>0.944034</td>\n","      <td>0.918605</td>\n","      <td>0.938689</td>\n","      <td>0.914376</td>\n","      <td>0.926879</td>\n","      <td>0.012047</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.491450</td>\n","      <td>0.051189</td>\n","      <td>0.928205</td>\n","      <td>0.047754</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.915523</td>\n","      <td>0.945090</td>\n","      <td>0.919662</td>\n","      <td>0.936575</td>\n","      <td>0.920719</td>\n","      <td>0.927514</td>\n","      <td>0.011337</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>19.766514</td>\n","      <td>2.769808</td>\n","      <td>1.078769</td>\n","      <td>0.042517</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.881732</td>\n","      <td>0.903907</td>\n","      <td>0.893235</td>\n","      <td>0.900634</td>\n","      <td>0.874207</td>\n","      <td>0.890743</td>\n","      <td>0.011243</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17.126656</td>\n","      <td>2.085552</td>\n","      <td>1.042649</td>\n","      <td>0.048023</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.892291</td>\n","      <td>0.931362</td>\n","      <td>0.900634</td>\n","      <td>0.915433</td>\n","      <td>0.889006</td>\n","      <td>0.905746</td>\n","      <td>0.015732</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.807970</td>\n","      <td>0.059128</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3.767048</td>\n","      <td>0.044233</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>14.029836</td>\n","      <td>2.607554</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>12.553254</td>\n","      <td>2.016797</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.925793</td>\n","      <td>0.076674</td>\n","      <td>0.879249</td>\n","      <td>0.049232</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.934530</td>\n","      <td>0.949314</td>\n","      <td>0.939746</td>\n","      <td>0.938689</td>\n","      <td>0.930233</td>\n","      <td>0.938502</td>\n","      <td>0.006366</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3.844805</td>\n","      <td>0.046670</td>\n","      <td>0.872198</td>\n","      <td>0.047547</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.933474</td>\n","      <td>0.950370</td>\n","      <td>0.938689</td>\n","      <td>0.935518</td>\n","      <td>0.934461</td>\n","      <td>0.938502</td>\n","      <td>0.006187</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>14.305696</td>\n","      <td>2.704589</td>\n","      <td>0.919353</td>\n","      <td>0.050746</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.922914</td>\n","      <td>0.937698</td>\n","      <td>0.926004</td>\n","      <td>0.931290</td>\n","      <td>0.911205</td>\n","      <td>0.925822</td>\n","      <td>0.008867</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12.591422</td>\n","      <td>2.013449</td>\n","      <td>0.911414</td>\n","      <td>0.047252</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.931362</td>\n","      <td>0.945090</td>\n","      <td>0.928118</td>\n","      <td>0.932347</td>\n","      <td>0.915433</td>\n","      <td>0.930470</td>\n","      <td>0.009484</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>3.780311</td>\n","      <td>0.044236</td>\n","      <td>0.872030</td>\n","      <td>0.047927</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.789863</td>\n","      <td>0.821542</td>\n","      <td>0.776956</td>\n","      <td>0.800211</td>\n","      <td>0.791755</td>\n","      <td>0.796065</td>\n","      <td>0.014754</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>3.702912</td>\n","      <td>0.055240</td>\n","      <td>0.868617</td>\n","      <td>0.049772</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.787751</td>\n","      <td>0.821542</td>\n","      <td>0.779070</td>\n","      <td>0.806554</td>\n","      <td>0.784355</td>\n","      <td>0.795854</td>\n","      <td>0.015836</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14.186411</td>\n","      <td>2.721489</td>\n","      <td>0.918875</td>\n","      <td>0.049614</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.898627</td>\n","      <td>0.925026</td>\n","      <td>0.901691</td>\n","      <td>0.917548</td>\n","      <td>0.890063</td>\n","      <td>0.906591</td>\n","      <td>0.012810</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>12.447004</td>\n","      <td>1.995039</td>\n","      <td>0.905357</td>\n","      <td>0.052795</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.899683</td>\n","      <td>0.922914</td>\n","      <td>0.905920</td>\n","      <td>0.922833</td>\n","      <td>0.900634</td>\n","      <td>0.910397</td>\n","      <td>0.010407</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n","0        6.616890      0.029710  ...        0.012047                5\n","1        6.491450      0.051189  ...        0.011337                4\n","2       19.766514      2.769808  ...        0.011243               10\n","3       17.126656      2.085552  ...        0.015732                9\n","4        3.807970      0.059128  ...             NaN               13\n","5        3.767048      0.044233  ...             NaN               14\n","6       14.029836      2.607554  ...             NaN               15\n","7       12.553254      2.016797  ...             NaN               16\n","8        3.925793      0.076674  ...        0.006366                1\n","9        3.844805      0.046670  ...        0.006187                1\n","10      14.305696      2.704589  ...        0.008867                6\n","11      12.591422      2.013449  ...        0.009484                3\n","12       3.780311      0.044236  ...        0.014754               11\n","13       3.702912      0.055240  ...        0.015836               12\n","14      14.186411      2.721489  ...        0.012810                8\n","15      12.447004      1.995039  ...        0.010407                7\n","\n","[16 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"JM8W2TTTX1TU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"347cd825-4692-456c-9253-39fc4dc6e056","executionInfo":{"status":"ok","timestamp":1579705111783,"user_tz":480,"elapsed":7900240,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}}},"source":["#print(\"Cell started\")\n","start = time.time()\n","\n","cachedir = mkdtemp()\n","memory = Memory(cachedir=cachedir, verbose=10)\n","print(\"Building Pipeline\")\n","\n","#not-remove_lemmatized\n","pipeline3 = Pipeline([\n","    ('vect', CountVectorizer(stop_words='english', tokenizer=lemmatized_tokenizer)),\n","    ('tfidf', TfidfTransformer()),\n","    ('reduce_dim', TruncatedSVD(n_components=50,random_state=0)),\n","    ('clf', GaussianNB()),\n","])\n","#not-remove_tokenizer\n","pipeline4 = Pipeline([\n","    ('vect', CountVectorizer(stop_words='english', tokenizer=tokenizer)),\n","    ('tfidf', TfidfTransformer()),\n","    ('reduce_dim', TruncatedSVD(n_components=50,random_state=0)),\n","    ('clf', GaussianNB()),\n","])\n","param_grid = [\n","     {\n","         'vect__min_df': min_df,\n","         'reduce_dim': [svd, nmf],\n","         'clf': [svm_clf, logistic_l1_clf, logistic_l2_clf, GaussianNB_clf]\n","     }\n","]\n","\n","\n","grid3 = GridSearchCV(pipeline3, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n","grid4 = GridSearchCV(pipeline4, cv=5, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n","\n","print(\"Fitting grid 3...\")\n","t1 = time.time()\n","# Fit KEPT HEADERS AND FOOTERS, LEMMITIZED\n","grid3.fit(all_train, target_train)\n","print(\"Fit grid 3 in %f sec\" % (time.time()-t1))\n","\n","print(\"Fitting grid 4...\")\n","t1 = time.time()\n","# Fit KEPT HEADERS AND FOOTERS, NOT LEMMITIZED\n","grid4.fit(all_train, target_train)\n","print(\"Fit grid 4 in %f sec\" % (time.time()-t1))\n","\n","pickle.dump( grid3, open(\"grid3.pkl\",\"wb\"))\n","pickle.dump( grid4, open(\"grid4.pkl\",\"wb\"))\n","\n","rmtree(cachedir)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n","You provided \"cachedir='/tmp/tmpf94uvl68'\", use \"location='/tmp/tmpf94uvl68'\" instead.\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Building Pipeline\n","Fitting grid 3...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"},{"output_type":"stream","text":["Fit grid 3 in 6611.734051 sec\n","Fitting grid 4...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n","ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n","\n","  FitFailedWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["Fit grid 4 in 1290.070636 sec\n","7902.023711681366\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ao6xXHwbZV1j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"79ea40d2-bd9d-4ddc-b1d1-8ac6dd6510a7","executionInfo":{"status":"ok","timestamp":1579705113011,"user_tz":480,"elapsed":1212,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}}},"source":["import pandas as pd\n","#no remove header, lemmatized\n","pd.DataFrame(grid3.cv_results_)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_clf</th>\n","      <th>param_reduce_dim</th>\n","      <th>param_vect__min_df</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>split4_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>64.181182</td>\n","      <td>0.920267</td>\n","      <td>15.444153</td>\n","      <td>1.095389</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.970433</td>\n","      <td>0.971489</td>\n","      <td>0.971459</td>\n","      <td>0.967230</td>\n","      <td>0.982030</td>\n","      <td>0.972528</td>\n","      <td>0.004999</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>63.533724</td>\n","      <td>1.108783</td>\n","      <td>15.341253</td>\n","      <td>1.036949</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.971489</td>\n","      <td>0.971489</td>\n","      <td>0.971459</td>\n","      <td>0.967230</td>\n","      <td>0.978858</td>\n","      <td>0.972105</td>\n","      <td>0.003756</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>84.775296</td>\n","      <td>1.483801</td>\n","      <td>15.486677</td>\n","      <td>1.058136</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.939810</td>\n","      <td>0.947202</td>\n","      <td>0.935518</td>\n","      <td>0.955603</td>\n","      <td>0.949260</td>\n","      <td>0.945478</td>\n","      <td>0.007087</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>78.452185</td>\n","      <td>2.158407</td>\n","      <td>15.555238</td>\n","      <td>1.053808</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.939810</td>\n","      <td>0.947202</td>\n","      <td>0.947146</td>\n","      <td>0.950317</td>\n","      <td>0.960888</td>\n","      <td>0.949073</td>\n","      <td>0.006843</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>61.796890</td>\n","      <td>1.043505</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>61.610356</td>\n","      <td>1.110927</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>79.313933</td>\n","      <td>1.255128</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>74.926600</td>\n","      <td>2.193917</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>62.216436</td>\n","      <td>0.994847</td>\n","      <td>15.337849</td>\n","      <td>1.080174</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.969377</td>\n","      <td>0.972545</td>\n","      <td>0.972516</td>\n","      <td>0.971459</td>\n","      <td>0.983087</td>\n","      <td>0.973797</td>\n","      <td>0.004786</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>61.669585</td>\n","      <td>1.066868</td>\n","      <td>15.280463</td>\n","      <td>1.086692</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.970433</td>\n","      <td>0.972545</td>\n","      <td>0.971459</td>\n","      <td>0.969345</td>\n","      <td>0.980973</td>\n","      <td>0.972951</td>\n","      <td>0.004149</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>78.883877</td>\n","      <td>1.426241</td>\n","      <td>15.372903</td>\n","      <td>0.979180</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.961985</td>\n","      <td>0.964097</td>\n","      <td>0.953488</td>\n","      <td>0.968288</td>\n","      <td>0.973573</td>\n","      <td>0.964286</td>\n","      <td>0.006696</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>73.993878</td>\n","      <td>2.236215</td>\n","      <td>15.393333</td>\n","      <td>1.048082</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.961985</td>\n","      <td>0.964097</td>\n","      <td>0.963002</td>\n","      <td>0.964059</td>\n","      <td>0.972516</td>\n","      <td>0.965132</td>\n","      <td>0.003773</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>61.515954</td>\n","      <td>1.101336</td>\n","      <td>15.289328</td>\n","      <td>1.125367</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.923970</td>\n","      <td>0.909187</td>\n","      <td>0.892178</td>\n","      <td>0.897463</td>\n","      <td>0.904863</td>\n","      <td>0.905532</td>\n","      <td>0.010929</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>61.420911</td>\n","      <td>0.973041</td>\n","      <td>15.288038</td>\n","      <td>1.083583</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.906019</td>\n","      <td>0.875396</td>\n","      <td>0.880550</td>\n","      <td>0.896406</td>\n","      <td>0.916490</td>\n","      <td>0.894972</td>\n","      <td>0.015352</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>79.391678</td>\n","      <td>1.468543</td>\n","      <td>15.506549</td>\n","      <td>0.990446</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.946146</td>\n","      <td>0.944034</td>\n","      <td>0.938689</td>\n","      <td>0.960888</td>\n","      <td>0.950317</td>\n","      <td>0.948015</td>\n","      <td>0.007448</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>74.261635</td>\n","      <td>1.951444</td>\n","      <td>15.423986</td>\n","      <td>1.041997</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.932418</td>\n","      <td>0.945090</td>\n","      <td>0.943975</td>\n","      <td>0.942918</td>\n","      <td>0.955603</td>\n","      <td>0.944001</td>\n","      <td>0.007364</td>\n","      <td>10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n","0       64.181182      0.920267  ...        0.004999                3\n","1       63.533724      1.108783  ...        0.003756                4\n","2       84.775296      1.483801  ...        0.007087                9\n","3       78.452185      2.158407  ...        0.006843                7\n","4       61.796890      1.043505  ...             NaN               13\n","5       61.610356      1.110927  ...             NaN               14\n","6       79.313933      1.255128  ...             NaN               15\n","7       74.926600      2.193917  ...             NaN               16\n","8       62.216436      0.994847  ...        0.004786                1\n","9       61.669585      1.066868  ...        0.004149                2\n","10      78.883877      1.426241  ...        0.006696                6\n","11      73.993878      2.236215  ...        0.003773                5\n","12      61.515954      1.101336  ...        0.010929               11\n","13      61.420911      0.973041  ...        0.015352               12\n","14      79.391678      1.468543  ...        0.007448                8\n","15      74.261635      1.951444  ...        0.007364               10\n","\n","[16 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"2IsZmC3ZSDZm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"16b6ba68-9b32-4b7c-d540-6ad482639854","executionInfo":{"status":"ok","timestamp":1579705113012,"user_tz":480,"elapsed":1204,"user":{"displayName":"PENG-YU CHEN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBatSluKAfMB_27PDSyU17dApOy6LRoVlNNwb--=s64","userId":"01601865878561972234"}}},"source":["#mo remove header, no lemmatized\n","pd.DataFrame(grid4.cv_results_)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_clf</th>\n","      <th>param_reduce_dim</th>\n","      <th>param_vect__min_df</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>split4_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8.546626</td>\n","      <td>0.063307</td>\n","      <td>1.425993</td>\n","      <td>0.048927</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.968321</td>\n","      <td>0.977825</td>\n","      <td>0.968288</td>\n","      <td>0.965116</td>\n","      <td>0.974630</td>\n","      <td>0.970836</td>\n","      <td>0.004664</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8.341619</td>\n","      <td>0.065242</td>\n","      <td>1.424892</td>\n","      <td>0.049552</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.970433</td>\n","      <td>0.976769</td>\n","      <td>0.967230</td>\n","      <td>0.966173</td>\n","      <td>0.978858</td>\n","      <td>0.971893</td>\n","      <td>0.005077</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30.626469</td>\n","      <td>2.381523</td>\n","      <td>1.670159</td>\n","      <td>0.033657</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.946146</td>\n","      <td>0.948258</td>\n","      <td>0.936575</td>\n","      <td>0.942918</td>\n","      <td>0.946089</td>\n","      <td>0.943997</td>\n","      <td>0.004084</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>24.574407</td>\n","      <td>1.556890</td>\n","      <td>1.600908</td>\n","      <td>0.052012</td>\n","      <td>SVC(C=1.0, break_ties=False, cache_size=200, c...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': SVC(C=1.0, break_ties=False, cache_siz...</td>\n","      <td>0.934530</td>\n","      <td>0.953537</td>\n","      <td>0.927061</td>\n","      <td>0.948203</td>\n","      <td>0.956660</td>\n","      <td>0.943998</td>\n","      <td>0.011362</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6.241829</td>\n","      <td>0.089456</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6.135799</td>\n","      <td>0.034844</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>24.226769</td>\n","      <td>2.370256</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>19.760971</td>\n","      <td>1.298014</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=10, class_weight=...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>6.273757</td>\n","      <td>0.041735</td>\n","      <td>1.388677</td>\n","      <td>0.046304</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.971489</td>\n","      <td>0.977825</td>\n","      <td>0.968288</td>\n","      <td>0.971459</td>\n","      <td>0.980973</td>\n","      <td>0.974006</td>\n","      <td>0.004661</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>6.190693</td>\n","      <td>0.081876</td>\n","      <td>1.394775</td>\n","      <td>0.036613</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.968321</td>\n","      <td>0.974657</td>\n","      <td>0.966173</td>\n","      <td>0.968288</td>\n","      <td>0.978858</td>\n","      <td>0.971259</td>\n","      <td>0.004746</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>23.294154</td>\n","      <td>2.253415</td>\n","      <td>1.472315</td>\n","      <td>0.040456</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.964097</td>\n","      <td>0.964097</td>\n","      <td>0.960888</td>\n","      <td>0.964059</td>\n","      <td>0.961945</td>\n","      <td>0.963017</td>\n","      <td>0.001349</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>19.711615</td>\n","      <td>1.255773</td>\n","      <td>1.470772</td>\n","      <td>0.044959</td>\n","      <td>LogisticRegression(C=100, class_weight=None, d...</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': LogisticRegression(C=100, class_weight...</td>\n","      <td>0.958817</td>\n","      <td>0.961985</td>\n","      <td>0.959831</td>\n","      <td>0.958774</td>\n","      <td>0.971459</td>\n","      <td>0.962173</td>\n","      <td>0.004787</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>6.150532</td>\n","      <td>0.053887</td>\n","      <td>1.388456</td>\n","      <td>0.045939</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.848997</td>\n","      <td>0.906019</td>\n","      <td>0.908034</td>\n","      <td>0.928118</td>\n","      <td>0.930233</td>\n","      <td>0.904280</td>\n","      <td>0.029377</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>6.082669</td>\n","      <td>0.087577</td>\n","      <td>1.389949</td>\n","      <td>0.043521</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>TruncatedSVD(algorithm='randomized', n_compone...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.910243</td>\n","      <td>0.904963</td>\n","      <td>0.897463</td>\n","      <td>0.917548</td>\n","      <td>0.922833</td>\n","      <td>0.910610</td>\n","      <td>0.008971</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>23.254269</td>\n","      <td>2.106661</td>\n","      <td>1.493304</td>\n","      <td>0.040485</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>3</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.934530</td>\n","      <td>0.953537</td>\n","      <td>0.946089</td>\n","      <td>0.935518</td>\n","      <td>0.951374</td>\n","      <td>0.944210</td>\n","      <td>0.007888</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>19.336161</td>\n","      <td>1.345304</td>\n","      <td>1.451517</td>\n","      <td>0.045496</td>\n","      <td>GaussianNB(priors=None, var_smoothing=1e-09)</td>\n","      <td>NMF(alpha=0.0, beta_loss='frobenius', init='ra...</td>\n","      <td>5</td>\n","      <td>{'clf': GaussianNB(priors=None, var_smoothing=...</td>\n","      <td>0.934530</td>\n","      <td>0.950370</td>\n","      <td>0.943975</td>\n","      <td>0.937632</td>\n","      <td>0.955603</td>\n","      <td>0.944422</td>\n","      <td>0.007804</td>\n","      <td>7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n","0        8.546626      0.063307  ...        0.004664                4\n","1        8.341619      0.065242  ...        0.005077                2\n","2       30.626469      2.381523  ...        0.004084               10\n","3       24.574407      1.556890  ...        0.011362                9\n","4        6.241829      0.089456  ...             NaN               13\n","5        6.135799      0.034844  ...             NaN               14\n","6       24.226769      2.370256  ...             NaN               15\n","7       19.760971      1.298014  ...             NaN               16\n","8        6.273757      0.041735  ...        0.004661                1\n","9        6.190693      0.081876  ...        0.004746                3\n","10      23.294154      2.253415  ...        0.001349                5\n","11      19.711615      1.255773  ...        0.004787                6\n","12       6.150532      0.053887  ...        0.029377               12\n","13       6.082669      0.087577  ...        0.008971               11\n","14      23.254269      2.106661  ...        0.007888                8\n","15      19.336161      1.345304  ...        0.007804                7\n","\n","[16 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Znb0anm0ofv1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}